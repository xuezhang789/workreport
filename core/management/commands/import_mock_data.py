
import os
import csv
import time
from datetime import datetime
from django.core.management.base import BaseCommand
from django.contrib.auth.models import User
from django.db import transaction, connection
from core.models import Profile
from projects.models import Project, ProjectPhaseConfig
from tasks.models import Task
from work_logs.models import DailyReport
from tqdm import tqdm
from django.conf import settings

# Increase CSV field size limit just in case
csv.field_size_limit(100 * 1024 * 1024)

class Command(BaseCommand):
    help = 'Import mock data from CSV files generated by scripts/generate_mock_data.py'

    def add_arguments(self, parser):
        parser.add_argument('--dir', type=str, default='scripts/mock_data_output', help='Directory containing CSV files')
        parser.add_argument('--batch-size', type=int, default=5000, help='Batch size for bulk_create')

    def handle(self, *args, **options):
        # Disable Debug to save memory on SQL logging
        settings.DEBUG = False
        
        data_dir = options['dir']
        batch_size = options['batch_size']
        
        if not os.path.exists(data_dir):
            self.stdout.write(self.style.ERROR(f"Directory {data_dir} does not exist."))
            return

        start_time = time.time()
        
        # 0. Cleanup (Optional, maybe dangerous? Let's just append or assume clean DB based on user request "Import to current system")
        # The user said "Import to current system", implying we should just add data.
        # But IDs might conflict if DB is not empty.
        # Since we just cleaned the project, it should be empty (except admin).
        
        self.import_users(data_dir, batch_size)
        self.import_profiles(data_dir, batch_size)
        self.import_phases(data_dir, batch_size)
        self.import_projects(data_dir, batch_size)
        self.import_project_members(data_dir, batch_size)
        self.import_tasks(data_dir, batch_size)
        self.import_reports(data_dir, batch_size)
        self.import_report_projects(data_dir, batch_size)
        
        duration = time.time() - start_time
        self.stdout.write(self.style.SUCCESS(f"All data imported in {duration:.2f} seconds."))

    def _read_csv(self, file_path):
        if not os.path.exists(file_path):
            self.stdout.write(self.style.WARNING(f"File {file_path} not found. Skipping."))
            return []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                yield row

    def _count_lines(self, file_path):
        if not os.path.exists(file_path):
            return 0
        with open(file_path, 'rb') as f:
            return sum(1 for _ in f) - 1

    def import_users(self, data_dir, batch_size):
        file_path = os.path.join(data_dir, 'users.csv')
        total = self._count_lines(file_path)
        if total == 0: return

        self.stdout.write(f"Importing Users ({total})...")
        objs = []
        for row in tqdm(self._read_csv(file_path), total=total):
            objs.append(User(
                id=row['id'],
                password=row['password'],
                last_login=row['last_login'] or None,
                is_superuser=int(row['is_superuser']),
                username=row['username'],
                first_name=row['first_name'],
                last_name=row['last_name'],
                email=row['email'],
                is_staff=int(row['is_staff']),
                is_active=int(row['is_active']),
                date_joined=row['date_joined']
            ))
            if len(objs) >= batch_size:
                User.objects.bulk_create(objs, ignore_conflicts=True)
                objs = []
        if objs:
            User.objects.bulk_create(objs, ignore_conflicts=True)

    def import_profiles(self, data_dir, batch_size):
        file_path = os.path.join(data_dir, 'profiles.csv')
        total = self._count_lines(file_path)
        if total == 0: return

        self.stdout.write(f"Importing Profiles ({total})...")
        objs = []
        for row in tqdm(self._read_csv(file_path), total=total):
            objs.append(Profile(
                id=row['id'],
                user_id=row['user_id'],
                position=row['position']
            ))
            if len(objs) >= batch_size:
                Profile.objects.bulk_create(objs, ignore_conflicts=True)
                objs = []
        if objs:
            Profile.objects.bulk_create(objs, ignore_conflicts=True)

    def import_phases(self, data_dir, batch_size):
        file_path = os.path.join(data_dir, 'project_phases.csv')
        total = self._count_lines(file_path)
        if total == 0: return

        self.stdout.write(f"Importing Project Phases ({total})...")
        objs = []
        for row in tqdm(self._read_csv(file_path), total=total):
            objs.append(ProjectPhaseConfig(
                id=row['id'],
                phase_name=row['phase_name'],
                progress_percentage=int(row['progress_percentage']),
                order_index=int(row['order_index']),
                is_active=int(row['is_active'])
            ))
        ProjectPhaseConfig.objects.bulk_create(objs, ignore_conflicts=True)

    def import_projects(self, data_dir, batch_size):
        file_path = os.path.join(data_dir, 'projects.csv')
        total = self._count_lines(file_path)
        if total == 0: return

        self.stdout.write(f"Importing Projects ({total})...")
        objs = []
        for row in tqdm(self._read_csv(file_path), total=total):
            objs.append(Project(
                id=row['id'],
                name=row['name'],
                code=row['code'],
                description=row['description'],
                start_date=row['start_date'] or None,
                end_date=row['end_date'] or None,
                owner_id=row['owner_id'],
                is_active=int(row['is_active']),
                current_phase_id=row['current_phase_id'],
                overall_progress=float(row['overall_progress']),
                created_at=row['created_at']
            ))
            if len(objs) >= batch_size:
                Project.objects.bulk_create(objs, ignore_conflicts=True)
                objs = []
        if objs:
            Project.objects.bulk_create(objs, ignore_conflicts=True)

    def import_project_members(self, data_dir, batch_size):
        file_path = os.path.join(data_dir, 'project_members.csv')
        total = self._count_lines(file_path)
        if total == 0: return

        self.stdout.write(f"Importing Project Members ({total})...")
        # Access the through model
        ThroughModel = Project.members.through
        objs = []
        for row in tqdm(self._read_csv(file_path), total=total):
            objs.append(ThroughModel(
                id=row['id'],
                project_id=row['project_id'],
                user_id=row['user_id']
            ))
            if len(objs) >= batch_size:
                ThroughModel.objects.bulk_create(objs, ignore_conflicts=True)
                objs = []
        if objs:
            ThroughModel.objects.bulk_create(objs, ignore_conflicts=True)

    def import_tasks(self, data_dir, batch_size):
        file_path = os.path.join(data_dir, 'tasks.csv')
        total = self._count_lines(file_path)
        if total == 0: return

        self.stdout.write(f"Importing Tasks ({total})...")
        objs = []
        for row in tqdm(self._read_csv(file_path), total=total):
            objs.append(Task(
                id=row['id'],
                title=row['title'],
                content=row['content'],
                user_id=row['user_id'],
                project_id=row['project_id'],
                category=row['category'],
                status=row['status'],
                priority=row['priority'],
                due_at=row['due_at'],
                created_at=row['created_at']
            ))
            if len(objs) >= batch_size:
                Task.objects.bulk_create(objs, ignore_conflicts=True)
                objs = []
        if objs:
            Task.objects.bulk_create(objs, ignore_conflicts=True)

    def import_reports(self, data_dir, batch_size):
        file_path = os.path.join(data_dir, 'reports.csv')
        total = self._count_lines(file_path)
        if total == 0: return

        self.stdout.write(f"Importing Reports ({total})...")
        objs = []
        for row in tqdm(self._read_csv(file_path), total=total):
            # Fill missing fields with defaults
            objs.append(DailyReport(
                id=row['id'],
                user_id=row['user_id'],
                date=row['date'],
                role=row['role'],
                status=row['status'],
                today_work=row['today_work'],
                tomorrow_plan=row['tomorrow_plan'],
                created_at=row['created_at'],
                updated_at=row['created_at'] # Set updated_at = created_at
            ))
            if len(objs) >= batch_size:
                DailyReport.objects.bulk_create(objs, ignore_conflicts=True)
                objs = []
        if objs:
            DailyReport.objects.bulk_create(objs, ignore_conflicts=True)

    def import_report_projects(self, data_dir, batch_size):
        file_path = os.path.join(data_dir, 'report_projects.csv')
        total = self._count_lines(file_path)
        if total == 0: return

        self.stdout.write(f"Importing Report Projects ({total})...")
        ThroughModel = DailyReport.projects.through
        objs = []
        for row in tqdm(self._read_csv(file_path), total=total):
            objs.append(ThroughModel(
                id=row['id'],
                dailyreport_id=row['dailyreport_id'],
                project_id=row['project_id']
            ))
            if len(objs) >= batch_size:
                ThroughModel.objects.bulk_create(objs, ignore_conflicts=True)
                objs = []
        if objs:
            ThroughModel.objects.bulk_create(objs, ignore_conflicts=True)
